# Compression Support in Reperio

## Overview

Reperio supports reading compressed Hadoop SequenceFiles directly, including those generated by Apache Nutch. Compression support is provided entirely by the `hadoop` Python library, which is already a project dependency.

## Supported Compression Codecs

All compression codecs supported by the hadoop library are available:

### DefaultCodec (DEFLATE/zlib)
- **Class**: `org.apache.hadoop.io.compress.DefaultCodec`
- **Status**: ✓ Fully supported
- **Description**: The most common compression codec in Hadoop/Nutch. Uses standard DEFLATE algorithm with zlib.
- **Use Case**: Default compression for Nutch LinkDB, CrawlDB when compression is enabled

### GzipCodec
- **Class**: `org.apache.hadoop.io.compress.GzipCodec`
- **Status**: ✓ Fully supported
- **Description**: Gzip compression format

### BZip2Codec
- **Class**: `org.apache.hadoop.io.compress.BZip2Codec`
- **Status**: ✓ Fully supported
- **Description**: BZip2 compression format

### Additional Codecs
- **SnappyCodec**: Supported if `python-snappy` is installed
- **Lz4Codec**: Supported if `lz4` is installed

## Implementation Details

### Simple Delegation to Hadoop Library

Reperio delegates all compression handling to the `hadoop` Python library:

1. **Auto-detection**: Reads SequenceFile header to detect compression
2. **Delegation**: If compressed, uses `hadoop.io.SequenceFile.Reader`
3. **Transparent**: Hadoop library handles decompression automatically
4. **Parsing**: Reperio receives decompressed bytes and parses as normal

This approach ensures:
- **Reliability**: Battle-tested hadoop library handles all edge cases
- **Simplicity**: No custom compression code to maintain
- **Comprehensive**: All Hadoop codecs supported automatically
- **Proven**: Same library used throughout the Hadoop ecosystem

### Architecture

```
SequenceFile Reader
│
├── Uncompressed files → Direct parsing
│
└── Compressed files → Hadoop library
                       ├── Auto-detects codec
                       ├── Decompresses transparently
                       └── Returns decompressed bytes
                              │
                              └──> Reperio parses as normal
```

### Code Structure

- **`reperio/readers/sequencefile_reader.py`**:
  - Auto-detects compression from SequenceFile headers
  - Uses `hadoop.io.SequenceFile.Reader` for compressed files
  - `_read_with_hadoop_library()`: Reads via hadoop library
  - Parses decompressed data with existing Nutch parsers

## Usage

### Reading Compressed Files

Compression support is automatic. Simply point Reperio at a compressed SequenceFile:

```bash
# Read compressed LinkDB
reperio stats /path/to/linkdb/part-r-00000/data --type linkdb

# Export compressed CrawlDB
reperio export /path/to/crawldb/part-r-00000/data output.json --type crawldb
```

### Checking Compression Status

```python
from reperio.readers.sequencefile_reader import NutchSequenceFileReader
from reperio.readers.filesystem import FileSystemManager

fs = FileSystemManager.create('/path/to/file')
reader = NutchSequenceFileReader('/path/to/file', 'linkdb', fs)

metadata = reader.get_metadata()
print(f"Compressed: {metadata['compression']}")
print(f"Codec: {metadata['compression_codec']}")
print(f"Block compression: {metadata['block_compression']}")

# The hadoop library handles decompression automatically
# You can read the file the same way regardless of compression
for record in reader.read_records(max_records=10):
    print(record)
```

## Testing

The implementation was tested with real Nutch data:

```bash
# Test with compressed LinkDB from Apache Nutch crawl
$ reperio stats ../../apache/nutch/crawldb/linkdb/current/part-r-00000/data \
    --type linkdb --max-records 10

Loading linkdb from: ../../apache/nutch/crawldb/linkdb/current/part-r-00000/data
Auto-detected: SequenceFile format
Parsing LinkDB...
       LinkDB Statistics       
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┓
┃ Metric              ┃ Value ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━┩
│ total_target_urls   │ 10    │
│ total_inlinks       │ ...   │
│ unique_source_urls  │ ...   │
└─────────────────────┴───────┘
✓ Done!
```

### Verification

Decompression verified against raw bytes:

```python
from hadoop.io import SequenceFile

reader = SequenceFile.Reader('compressed_file.seq')
key_buffer = reader.nextRawKey()
value_buffer = reader.nextRawValue()

# Successfully decompresses and returns raw bytes
key_bytes = key_buffer.toByteArray()    # Decompressed key
value_bytes = value_buffer.toByteArray()  # Decompressed value

# Example output:
# URL: http://ant.apache.org/ivy/
# Decompressed value size: 66 bytes
```

## Compression Format Details

### Record Compression
- **Format**: `[record_length][key_length][key][compressed_value]`
- **Notes**: 
  - Keys are NOT compressed
  - Only values are compressed
  - Most common format in Nutch

### Block Compression
- **Format**: More complex block structure
- **Status**: Supported via hadoop library
- **Notes**: Groups multiple records into compressed blocks

## Performance Considerations

### Memory Usage
- Compressed files are read sequentially
- Decompression happens on-demand
- Memory usage proportional to individual record size, not total file size

### Speed
- Decompression adds overhead (~10-30% slower than uncompressed)
- Trade-off: Significantly smaller file sizes
- Typical compression ratio: 2-5x for text data

### Recommendations
- Use compression for long-term storage
- Consider uncompressed for frequently-read data
- Block compression offers better compression but slower random access

## Troubleshooting

### "Compressed SequenceFiles require the 'hadoop' library"
- **Cause**: Hadoop library not installed
- **Solution**: Install with `pip install -e .` (hadoop is in requirements.txt)

### "Compression codec not supported" or codec-specific errors
- **Cause**: Missing optional codec libraries (Snappy, LZ4)
- **Solution**: 
  - For Snappy: `pip install python-snappy`
  - For LZ4: `pip install lz4`
  - Or use Nutch to export to text format first

### Incomplete decompression errors
- **Cause**: Corrupted compressed data or incomplete file transfer
- **Solution**: Verify file integrity with checksum, re-copy from HDFS if needed

## Future Enhancements

Potential improvements for future versions:

1. **Performance Optimizations**:
   - Parallel reading of multi-part SequenceFiles
   - Streaming decompression for very large files
   - Progress indicators for long-running operations

2. **Additional Features**:
   - Compression statistics (ratios, codec detection)
   - Auto-recommend optimal codec based on file analysis
   - Write support (compress when exporting)

## References

- [Hadoop SequenceFile Format](https://hadoop.apache.org/docs/current/api/org/apache/hadoop/io/SequenceFile.html)
- [Apache Nutch Data Formats](https://nutch.apache.org/)
- [Python zlib documentation](https://docs.python.org/3/library/zlib.html)
